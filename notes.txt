MNIST simulation in C++

============
4/25/2017
So for now I am going to simulate a recurrent Neural
Network that runs the benchmark MNIST handwritten
number catagorization test. I'm making this git
repository so I am able to work in the lab as well
as from the dorm or the library. The steps I will
take to complete this will be as follows. 
1.) Design solution on paper and review past work
2.) make program

Obviously there will be parts 1.1 and 2.1 but thats
a later problem.

============
5/8/2017
Beginning to create the NN in C++ initially focusing
on the ability to read the appropreate MNIST data
set and return the information in array structures
that I can process accordingly. Still re-learning
the versatilites of vim and will need to either 
write my own matrix math functions or if I can
leverage MATLAB and its highly developed matrix lib.

Would like to finish the research and the Stanford
CS231n Convolutional Neural Networks online lecture
notes analysis by this Thursday and fully understood
my end goal design by this Friday. Today though I
want to have the read function and finish many notes

============
5/9/2017
Learned more of the basics yesterday of construction
of my neural net, and I'll look to implement some
cool features as well as some nessacary ones. Really
need to hone down today on how I am going to handle
my matrix algebra. Most likely will use Eigen as it
has very little setup and has lots of support info
online. Today will also try to address how to read
mnist file.

Design update:: 
My original idea of placing all of my code within
one call function may be misguided and I'll try to 
approach the problem from a different point of view.
I will be making two programs. One that creates and
teaches a neural network from random weights and 
trains those weights to to MNIST dataset, then
records those datapoints into a seperate file 
(most likely binary) then run a second program that
constructs a neural network with priviously 
generated weights and test the accuracy of those
weights on the test MNIST dataset, outputting a 
percent error readout.::

===========
5/11/2017
To complete today: finish MNIST dataset reader 
functions to be used in both test and train 
processes. If that gets finished then begin first
layer of train neural net. Finished readings on 
gradient decent and calulating loss functions so
hopefully implentation of that today can happen.


good resource, and develop python mnist to better
understand:
neuralnetworksanddeeplearning.com

============
5/12/2017
Got system set up on home computer, can now work
from home. Still not sure if thats an acceptable
way to do work for this Japanese work lab
environment. Today finishing up reading mnist
piece. Also get neural net flow by making program
in python with numpy linear algebra engine.

================
5/15/2017
Today will have finished reader function and will
try to get output clear and effective. 
The goal will be to work out the nuances of the 
way EIGEN works and the ability to effectivly 
use the eigen::matrixXd object for linear algebra
for the simulation of a neural layer, datastream
and output. If I get there i may finish up by
starting the baseline design of the training
program and the command line inputs I will be 
requiring and using. If I'm good enough I might 
just make it one program that has two different 
input flags that indicate both training and
testing.

================
5/16/2017
Re-factoring the way to handle the mnist_block
class so that smoother setters and getters. The
'this' usage needs to be redone into usable
functions.

================
5/17/2017
Pulled most recent version of project from
github and continued to work on the main file
with a focus on the flag functionality. Start
designing the integration of the network and 
the way it works within itself. 

================
5/19/2017
In an effort to work in an object oriented
way I must rethink the interaction that the mnist
reader class is used. I would like to just have
two instances, one with testing data and one with
training data, but with the duality of a program that
can do testing and training two different ways with
the just a change of flag may make that a problem.
If I integrate that in the lowest level though then
I could abstract that out and the extra data should
not be a yuge slow down. It is just a factor of o(1)
of an increase and force my process to use a lot more
data than probably expected. Ultimately the functions
with NN specific  functions, gradient decent, feed
forward processes and stuff like backpropogation
need to be slightly better understood for me to be
able to effectively integrate a network. Also if I
sit down and design the network out a bit better in
terms of what layers do what and how many I want will
be good in planning out the project layout. This
weekend I will most likely be doing japanese but I
will finish some reading on networks from the book 
that sensei lent me. 

==================
5/22/2017
Details of NN equations derivations are clearer, and
easier to understand after a weekend of thinking
over them. Implementation on a conceptual level makes
practical sense but the nuances of Eigen types (matrix
transposition, managemennt, access and save) and ood
classification techniques, I will still need to hone
out. I want to finish the engine class that should
only really provide the functions. On top of that
all the data, matrix and managemet work should be 
done by inheretance.

==================
5/25/2017
Building up the read driver which will take care of 
the reading and writing to files. I'm goin to also
use it to be in charge of the top level weight 
randomization, file creation and info validation.
Will only check for corruption but will not check
for correct matrix size or matrix values. Will
be a black box for the nn_engine and will just be 
used at the beginning and the end to set the matrix
pointers. ALL WEIGHT MATRICIES WILL BE CREATED IN
THIS OBJECT. THIS CLASS WILL ONLY BE PASSED ALREADY
EXISTING MATRIX POINTERS ON A WRITE OPERATION. Idk 
maybe this isn't the correct way to go about it but
on paper and in my head it seem like it will work.
I am almost at the point where all my tools will be
in front of me and I can implement the network 
math. Should not be that hard, biggest issues I'm 
expecting to run into are segfaults caused by
pointer mishandling, transpose operation (please
god let eigen have a method) and batch management.

==================
5/29/2017
Worked a bit over the weekend so now I have unit
tests for the mnist block class which passes in
the current state as well as I have a test that
runs through data randomization and printing it 
to a .txt file visually. I'll have the read
portion done tomorrow and after that I will
implement the actuall network :)) yippee!! cool
that I'm almost there and once I am I expect
a good number of issues but I have a very good
working knowledge of neural networks at this 
point and any issue I run into theoretically I
can find the answer to it somehow. Also I think
I have regained my skills on C++ as well so
things should begin to run quit a bit smoother
now. Today had a bit of a run in with pointers
but I got that figured out. Pointers to pointers
are a thing of beauty. Japanese test wednesday. 
god rest my soul. 

===================
5/302017
Tuesday and while my mind is mostly concerned
with the test tomorrow, I think I should set
a finish by today deadline of the weight reader
that reads in data from a txt file. Once I have
that method the rest of the project can proceed
as planned. The work involved with the network
can start and I hope to have that going at least 
by this Friday, maybe not necessarially any of
the core math but setting up the matrices and 
vectors to be passed will be crucial. I may need
to add an "average" vector to the intermediate
holder vectors so that I can successfully run
backpropogation. We will cross that bridge when
we arrive at it though. For today, I will finish
the reader. may be a bit slow but by doing it
char by char I can have the most accurate data.
If not char by char I will update this log to 
reflect that.

===================
6/1/2017
So failed the test. That was fun. Moving on to 
what I enjoy more though I will be making setters
and getters for the nn_engine class and some of
the core forward pass math work along with the
various ReLU and derivitive ReLU functions.
Hopefully Ill be able to make a unit test for 
a forward pass operation. Also need to make 
softmax classifier for 10 unit output vector.
I'm not expecting any of these operations to 
be all that difficult but I need to figure out
how I want inheritance to work into the equation.
Eventually I would like to see the main call be
brain.setBatchSize(x);
brain.numberEpoch(x);
brain.runTrain();
brain.evaluate();
where runTrain will print out progress and output
a weight .txt doc at the end and then evaluate
will return an accuracy percent.

======================
6/2/2017
Today the goal is to get the backprop algorithm
down. For the engine just have the ability to 
return a matrix for each weight matrix of the 
gradient decent and spit out what that value is.
Next layer of abstraction will be evaluating 
that and applying a step value to the gradient. 
then applying that to the weights. Today still
just laying groundwork on the nn_engine and 
adding functions to the backbone for more abstract
classes can utilize. 

======================
6/5/2017
Finished all the back end work for the network
now I just have to set up the front end command
line handling. I will finish the program by today
and it will will either train or test but not
run whole analysis. It will either be testing or 
training. Completed the neural_controller and 
neural_engine. Working on the nn_driver main
function and will build in appropriate components
of that. Will mostly ignore or not work on the 
unit tests. That can be a "for me" project. Just
getting the program to run will be what I need to 
do for the lab. Will hopefully be moving on to 
FPGA by the end of this week. Or at least begin my
background research on the topic.
 
